{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa40d4ad",
   "metadata": {},
   "source": [
    "Codeblock zum importieren aller dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20acf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import logging\n",
    "import subprocess\n",
    "from openai import OpenAI, OpenAIError\n",
    "import difflib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36630b",
   "metadata": {},
   "source": [
    "Definition der Funktion gett_llms_from_api\n",
    "-> Gibt eine Liste der über die API verfügbaren LLMs an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1356095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llms_from_api():\n",
    "    \"\"\"\n",
    "    Get all LLMs from the API.\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "        api_key = \"f00b07316f6f7f5e2f1519e7be703dba\", base_url=\"https://chat-ai.academiccloud.de/v1/\"\n",
    "    )\n",
    "    response = client.models.list()\n",
    "    llms = []\n",
    "    for model in response.data:\n",
    "        llms.append(\n",
    "            {\n",
    "                \"id_name\": model.id,\n",
    "                \"short_name\": model.name,\n",
    "            }\n",
    "        )\n",
    "    return llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eff3121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id_name': 'meta-llama-3.1-8b-instruct',\n",
       "  'short_name': 'Meta Llama 3.1 8B Instruct'},\n",
       " {'id_name': 'openai-gpt-oss-120b', 'short_name': 'OpenAI GPT OSS 120B'},\n",
       " {'id_name': 'gemma-3-27b-it', 'short_name': 'Gemma 3 27B Instruct'},\n",
       " {'id_name': 'qwen3-32b', 'short_name': 'Qwen 3 32B'},\n",
       " {'id_name': 'qwen3-235b-a22b', 'short_name': 'Qwen 3 235B A22B 2507'},\n",
       " {'id_name': 'llama-3.3-70b-instruct',\n",
       "  'short_name': 'Meta Llama 3.3 70B Instruct'},\n",
       " {'id_name': 'qwen2.5-vl-72b-instruct',\n",
       "  'short_name': 'Qwen 2.5 VL 72B Instruct'},\n",
       " {'id_name': 'medgemma-27b-it', 'short_name': 'MedGemma 27B Instruct'},\n",
       " {'id_name': 'qwq-32b', 'short_name': 'Qwen QwQ 32B'},\n",
       " {'id_name': 'deepseek-r1', 'short_name': 'DeepSeek R1 0528'},\n",
       " {'id_name': 'deepseek-r1-distill-llama-70b',\n",
       "  'short_name': 'DeepSeek R1 Distill Llama 70B'},\n",
       " {'id_name': 'mistral-large-instruct', 'short_name': 'Mistral Large Instruct'},\n",
       " {'id_name': 'qwen2.5-coder-32b-instruct',\n",
       "  'short_name': 'Qwen 2.5 Coder 32B Instruct'},\n",
       " {'id_name': 'internvl2.5-8b', 'short_name': 'InternVL2.5 8B MPO'},\n",
       " {'id_name': 'teuken-7b-instruct-research',\n",
       "  'short_name': 'Teuken 7B Instruct Research'},\n",
       " {'id_name': 'codestral-22b', 'short_name': 'Codestral 22B'},\n",
       " {'id_name': 'llama-3.1-sauerkrautlm-70b-instruct',\n",
       "  'short_name': 'Llama 3.1 SauerkrautLM 70B Instruct'},\n",
       " {'id_name': 'meta-llama-3.1-8b-rag', 'short_name': 'Meta Llama 3.1 8B RAG'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llms_from_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cadf86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logger\n",
    "logger = logging.getLogger('test_logger')\n",
    "logger.setLevel(logging.DEBUG) # Set logging level to DEBUG for more detailed output\n",
    "\n",
    "def get_llm_response(\n",
    "    messages: list,\n",
    "    model=\"meta-llama-3.1-8b-instruct\",\n",
    "    temperature=0.7,\n",
    "    key=None,\n",
    "    max_tokens=1000,\n",
    "    url=None,\n",
    "):\n",
    "    logger.debug(\"Starting get_llm_response function.\")\n",
    "\n",
    "    # Set API credentials\n",
    "    try:\n",
    "        logger.debug(\"Attempting to get API key and URL.\")\n",
    "        if key is None:\n",
    "            key = \"f00b07316f6f7f5e2f1519e7be703dba\"\n",
    "            logger.debug(f\"Retrieved key from userdata: {'Key found' if key else 'Key not found'}\")\n",
    "        else:\n",
    "            logger.debug(\"Key provided directly.\")\n",
    "\n",
    "        if url is None:\n",
    "            url = \"https://chat-ai.academiccloud.de/v1/\" # Replace with your env var name\n",
    "            logger.debug(f\"Using default URL: {url}\")\n",
    "        else:\n",
    "             logger.debug(f\"URL provided directly: {url}\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"KeyError when getting API key or URL: {e}\")\n",
    "        return \"I'm sorry, but I couldn't process your request.\", {}\n",
    "\n",
    "    if not key:\n",
    "        logger.error(\"API key is missing.\")\n",
    "        return \"I'm sorry, but the API key is not set.\", {}\n",
    "\n",
    "    logger.debug(f\"API Key status: {'Set' if key else 'Not Set'}\")\n",
    "    logger.debug(f\"Base URL: {url}\")\n",
    "\n",
    "    # Initialize client\n",
    "    try:\n",
    "        logger.debug(\"Initializing OpenAI client.\")\n",
    "        client = OpenAI(\n",
    "            api_key=key,\n",
    "            base_url=url,\n",
    "            timeout=httpx.Timeout(60.0, connect=10.0)\n",
    "        )\n",
    "        logger.debug(\"OpenAI client initialized successfully.\")\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Error initializing OpenAI client: {e}\")\n",
    "         return \"I'm sorry, there was an error initializing the API client.\", {}\n",
    "\n",
    "\n",
    "    metadata = {}\n",
    "\n",
    "    # Make API request\n",
    "    try:\n",
    "        logger.debug(\"Making API request.\")\n",
    "        start_time = time.perf_counter()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        duration = end_time - start_time\n",
    "        logger.debug(\"API request completed.\")\n",
    "\n",
    "        # Extract response data\n",
    "        response_content = response.choices[0].message.content\n",
    "        prompt_tokens = response.usage.prompt_tokens if response.usage else 'N/A'\n",
    "        generated_tokens = response.usage.completion_tokens if response.usage else 'N/A'\n",
    "\n",
    "        # Log information\n",
    "        logger.info(f\"LLM response time: {duration:.4f} seconds\")\n",
    "        logger.debug(f\"Prompt tokens: {prompt_tokens}, Generated tokens: {generated_tokens}\")\n",
    "\n",
    "        # Create metadata dictionary\n",
    "        metadata = {\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"generated_tokens\": generated_tokens,\n",
    "            \"duration\": duration,\n",
    "        }\n",
    "        logger.debug(\"Metadata created.\")\n",
    "\n",
    "        return response_content, metadata\n",
    "    except OpenAIError as e:\n",
    "        logger.error(f\"An OpenAIError occurred while calling the LLM API: {e}\")\n",
    "        return \"I'm sorry, but I couldn't process your request due to an API error.\", metadata\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during the API call: {e}\")\n",
    "        return \"I'm sorry, an unexpected error occurred.\", metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded97c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting recording for 15 seconds...\n",
      "Recording saved successfully as meeting.wav\n",
      "FFmpeg output:\n",
      "\n",
      "ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.101 / 61. 19.101\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "2025-09-03 10:36:03.321 ffmpeg[97961:1201988] WARNING: Add NSCameraUseContinuityCameraDeviceType to your Info.plist to use AVCaptureDeviceTypeContinuityCamera.\n",
      "2025-09-03 10:36:03.579 ffmpeg[97961:1201988] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "Input #0, avfoundation, from ':0':\n",
      "  Duration: N/A, start: 50189.794000, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_f32le, 48000 Hz, mono, flt, 1536 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_f32le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'meeting.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf61.7.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, mono, s16, 768 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.19.101 pcm_s16le\n",
      "size=      42KiB time=00:00:00.51 bitrate= 673.2kbits/s speed=1.02x    \n",
      "size=      83KiB time=00:00:01.00 bitrate= 678.7kbits/s speed=0.994x    \n",
      "size=     125KiB time=00:00:01.51 bitrate= 676.5kbits/s speed=   1x    \n",
      "size=     167KiB time=00:00:02.02 bitrate= 675.3kbits/s speed=   1x    \n",
      "size=     209KiB time=00:00:02.52 bitrate= 677.5kbits/s speed=   1x    \n",
      "size=     250KiB time=00:00:03.02 bitrate= 676.3kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:03.53 bitrate= 594.0kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:04.03 bitrate= 520.1kbits/s speed=0.999x    \n",
      "size=     256KiB time=00:00:04.54 bitrate= 461.5kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:05.04 bitrate= 415.7kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:05.54 bitrate= 378.1kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:06.05 bitrate= 346.1kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:06.54 bitrate= 640.4kbits/s speed=0.999x    \n",
      "size=     512KiB time=00:00:07.07 bitrate= 593.1kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:07.57 bitrate= 553.8kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:08.07 bitrate= 519.4kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:08.57 bitrate= 489.1kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:09.07 bitrate= 462.1kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:09.57 bitrate= 656.8kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:10.09 bitrate= 623.5kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:10.59 bitrate= 594.0kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:11.09 bitrate= 567.1kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:11.59 bitrate= 542.6kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:12.10 bitrate= 519.7kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:12.60 bitrate= 665.3kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:13.10 bitrate= 639.9kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:13.61 bitrate= 616.3kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:14.12 bitrate= 594.0kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:14.62 bitrate= 573.6kbits/s speed=   1x    \n",
      "[out#0/wav @ 0x11c762000] video:0KiB audio:1253KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.006078%\n",
      "size=    1253KiB time=00:00:15.00 bitrate= 684.5kbits/s speed=0.999x    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def record_audio(output_filename, duration=15):\n",
    "    \"\"\"\n",
    "    Records audio from the microphone for a given duration and saves it to a file.\n",
    "\n",
    "    Args:\n",
    "        output_filename (str): The name of the output .wav file.\n",
    "        duration (int): The duration of the recording in seconds.\n",
    "    \"\"\"\n",
    "    print(f\"Starting recording for {duration} seconds...\")\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-f', 'avfoundation',  # Use AVFoundation for macOS\n",
    "        '-i', ':0',            # Select the default audio device\n",
    "        '-t', str(duration),   # Set the recording duration\n",
    "        '-y',                  # Overwrite output file if it exists\n",
    "        output_filename\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Recording saved successfully as {output_filename}\")\n",
    "        print(\"FFmpeg output:\")\n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred during recording: {e}\")\n",
    "        print(\"FFmpeg output (stderr):\")\n",
    "        print(e.stderr)\n",
    "    except FileNotFoundError:\n",
    "        print(\"ffmpeg not found. Please ensure ffmpeg is installed and in your PATH.\")\n",
    "\n",
    "# Record 10 seconds of audio and save it as 'meeting.wav'\n",
    "record_audio('meeting.wav', duration=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1845cf52",
   "metadata": {},
   "source": [
    "## Transcribe Audio with STT Service\n",
    "\n",
    "The following cell defines a function to send an audio file to the speech-to-text (STT) service running in a Docker container and retrieves the transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fccf0a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription Result:\n",
      "{'transcription': ' Das macht der Webserver.  Der Webserver ließ die Eingeworfenbriefe,  Request und beantwortet sie mit der Website,  Respaunt.  Um alles ausliefern zu können, brauchen wir Inhalte.  Und Django hilft dir dabei, diese Inhalte zu erstellen.', 'language': 'de', 'confidence': 0.9920963048934937}\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(file_path):\n",
    "    \"\"\"\n",
    "    Sends an audio file to the STT service and returns the transcription.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the audio file.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Attempting to transcribe audio file: {file_path}\")\n",
    "    \n",
    "    # The URL of the transcription service in the Docker container\n",
    "    transcribe_url = \"http://localhost:8080/transcribe/file\"\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"Audio file not found at: {file_path}\")\n",
    "            return \"Error: Audio file not found.\"\n",
    "\n",
    "        with open(file_path, \"rb\") as audio_file:\n",
    "            files = {\"file\": (os.path.basename(file_path), audio_file, \"audio/wav\")}\n",
    "            \n",
    "            logger.debug(f\"Sending POST request to {transcribe_url}\")\n",
    "            with httpx.Client() as client:\n",
    "                response = client.post(transcribe_url, files=files, timeout=60)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            transcription = response.json()\n",
    "            logger.info(\"Successfully received transcription.\")\n",
    "            logger.debug(f\"Transcription result: {transcription}\")\n",
    "            return transcription\n",
    "\n",
    "    except httpx.RequestError as e:\n",
    "        logger.error(f\"An error occurred while requesting transcription: {e}\")\n",
    "        return f\"Error connecting to the transcription service: {e}\"\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        logger.error(f\"Received an HTTP error: {e.response.status_code} - {e.response.text}\")\n",
    "        return f\"HTTP Error: {e.response.status_code}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "#transcribe the audio recording\n",
    "audio_to_transcribe = 'meeting.wav'\n",
    "transcription_result = transcribe_audio(audio_to_transcribe)\n",
    "\n",
    "print(\"Transcription Result:\")\n",
    "print(transcription_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "770fe382",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = '''Um besser zu verstehen, welche Vorteile dir Django bietet, werfen wir einen Blick auf Server im Allgemeinen. \n",
    "Als Erstes muss der Server wissen, dass er eine Webseite ausliefern soll. Der Server hat mehrere \"Ports\". \n",
    "Ein Port ist vergleichbar mit einem Briefkasten, der auf eingehende Briefe antwortet.'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4631eac",
   "metadata": {},
   "source": [
    "## Compare Original Text with Transcription\n",
    "\n",
    "The following cell uses Python's `difflib` library to compare the original text with the transcribed text and highlights the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ecc6f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-level differences between original text and transcription:\n",
      " dass\n",
      " er\n",
      " eine\n",
      "-Webseite\n",
      "+Website\n",
      " ausliefern\n",
      " soll.\n",
      " Der\n",
      " Server\n",
      " hat\n",
      " mehrere\n",
      "-\"Ports\".\n",
      "+Port.\n",
      " Ein\n",
      "-Port\n",
      "-ist\n",
      "-vergleichbar\n",
      "+Portist\n",
      "+vergleicht\n",
      "+wir\n",
      " mit\n",
      " einem\n",
      " Briefkasten,\n",
      " der\n",
      " auf\n",
      "-eingehende\n",
      "+eigentlichende\n",
      " Briefe\n",
      " antwortet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The transcribed text is inside the 'transcription' key of the result dictionary\n",
    "transcribed_text = transcription_result.get('transcription', '')\n",
    "\n",
    "# Split the texts into words for comparison\n",
    "original_words = original_text.split()\n",
    "transcribed_words = transcribed_text.split()\n",
    "\n",
    "# Create a diff object for word-level comparison\n",
    "diff = difflib.unified_diff(\n",
    "    original_words,\n",
    "    transcribed_words,\n",
    "    fromfile='original_text',\n",
    "    tofile='transcribed_text',\n",
    "    lineterm='',\n",
    ")\n",
    "\n",
    "# Print the word-level differences\n",
    "print(\"Word-level differences between original text and transcription:\")\n",
    "for line in diff:\n",
    "    # We can ignore the file headers from the diff\n",
    "    if not line.startswith('---') and not line.startswith('+++') and not line.startswith('@@'):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b466b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korrektur:\n",
      "{'transcription': ' Um besser zu verstehen, welche Vorteile dir Django bietet, werfen wir einen Blick auf Server im Allgemeinen.  Als erstes muss der Server wissen, dass er eine Website ausliefern soll.  Der Server hat mehrere Ports.  Ein Port ist vergleichbar mit einem Briefkasten, der auf bestimmte Briefe antwortet.', 'language': 'de', 'confidence': 0.9870548248291016}\n",
      "{'prompt_tokens': 172, 'generated_tokens': 101, 'duration': 1.3659923750019516}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Erstellen Sie die Nachrichten für die LLM-API\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Sie sind ein hilfreicher Assistent, der Texte korrigiert.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Korrigiere den folgenden Text und gebe nur den korrigierten Text aus:\\n\\n{transcription_result}\"}\n",
    "]\n",
    "\n",
    "# Rufen Sie die get_llm_response Funktion auf (aus der Zelle oben)\n",
    "correction, metadata = get_llm_response(messages)\n",
    "\n",
    "# Geben Sie die Korrektur aus\n",
    "print(\"Korrektur:\")\n",
    "print(correction)\n",
    "print(metadata)\n",
    "\n",
    "# Optional: Metadaten anzeigen\n",
    "# print(\"\\nMetadaten:\")\n",
    "# display(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2dafc19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-level differences between original text and corrected text:\n",
      "+{'transcription':\n",
      "+'\n",
      " Um\n",
      " besser\n",
      " zu\n",
      " im\n",
      " Allgemeinen.\n",
      " Als\n",
      "-Erstes\n",
      "+erstes\n",
      " muss\n",
      " der\n",
      " Server\n",
      " dass\n",
      " er\n",
      " eine\n",
      "-Webseite\n",
      "+Website\n",
      " ausliefern\n",
      " soll.\n",
      " Der\n",
      " Server\n",
      " hat\n",
      " mehrere\n",
      "-\"Ports\".\n",
      "+Ports.\n",
      " Ein\n",
      " Port\n",
      " ist\n",
      " Briefkasten,\n",
      " der\n",
      " auf\n",
      "-eingehende\n",
      "+bestimmte\n",
      " Briefe\n",
      "-antwortet.\n",
      "+antwortet.',\n",
      "+'language':\n",
      "+'de',\n",
      "+'confidence':\n",
      "+0.9870548248291016}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the texts into words for comparison\n",
    "original_words = original_text.split()\n",
    "corrected_words = correction.split()\n",
    "\n",
    "# Create a diff object for word-level comparison\n",
    "diff = difflib.unified_diff(\n",
    "    original_words,\n",
    "    corrected_words,\n",
    "    fromfile='original_text',\n",
    "    tofile='corrected_text',\n",
    "    lineterm='',\n",
    ")\n",
    "\n",
    "# Print the word-level differences\n",
    "print(\"Word-level differences between original text and corrected text:\")\n",
    "for line in diff:\n",
    "    # We can ignore the file headers from the diff\n",
    "    if not line.startswith('---') and not line.startswith('+++') and not line.startswith('@@'):\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "472aad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:\n",
      "Ein Server benötigt Informationen, um eine Website auszuliefern. Er hat verschiedene Ports, die wie Briefkästen funktionieren, die auf bestimmte Anfragen reagieren.\n",
      "{'prompt_tokens': 162, 'generated_tokens': 41, 'duration': 1.1010920840017207}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Erstellen Sie die Nachrichten für die LLM-API\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Sie sind ein hilfreicher Assistent, der Texte zusammenfasst.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Fassen Sie den folgenden Text zusammen:\\n\\n {transcription_result} \"}\n",
    "]\n",
    "\n",
    "# Rufen Sie die get_llm_response Funktion auf (aus der Zelle oben)\n",
    "summary, metadata = get_llm_response(messages)\n",
    "\n",
    "# Geben Sie die Zusammenfassung aus\n",
    "print(\"Zusammenfassung:\")\n",
    "print(summary)\n",
    "print(metadata)\n",
    "\n",
    "# Optional: Metadaten anzeigen\n",
    "# print(\"\\nMetadaten:\")\n",
    "# display(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c48c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Summary:\n",
      "Zusammenfassung:\n",
      "\n",
      "Ein Webserver ist für die Verbindung zwischen Benutzer und Website verantwortlich. Er nimmt Anfragen (Request) entgegen und antwortet mit der Website (Response). Um die Website bereitzustellen, benötigt der Webserver jedoch Inhalte, die durch eine Framework wie Django erstellt werden können. Django hilft bei der Erstellung von Inhalten, um die Website nutzbar zu machen.\n"
     ]
    }
   ],
   "source": [
    "# Initialize rolling summary\n",
    "rolling_summary = \"\"\n",
    "\n",
    "# After each new transcription, update the rolling summary\n",
    "def update_rolling_summary(new_transcription, previous_summary):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent, der fortlaufende Zusammenfassungen erstellt.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Vorherige Zusammenfassung:\\n{previous_summary}\\n\\nNeue Nachricht:\\n{new_transcription}\\n\\nFasse alles zusammen.\"}\n",
    "    ]\n",
    "    summary, metadata = get_llm_response(messages)\n",
    "    return summary\n",
    "\n",
    "# Example usage after getting a new transcription\n",
    "new_transcription = transcription_result.get('transcription', '')\n",
    "rolling_summary = update_rolling_summary(new_transcription, rolling_summary)\n",
    "\n",
    "print(\"Rolling Summary:\")\n",
    "print(rolling_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
