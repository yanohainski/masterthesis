{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c20acf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import logging\n",
    "import subprocess\n",
    "from openai import OpenAI, OpenAIError\n",
    "import difflib\n",
    "import threading\n",
    "from queue import Queue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1356095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id_name': 'meta-llama-3.1-8b-instruct',\n",
       "  'short_name': 'Meta Llama 3.1 8B Instruct'},\n",
       " {'id_name': 'openai-gpt-oss-120b', 'short_name': 'OpenAI GPT OSS 120B'},\n",
       " {'id_name': 'gemma-3-27b-it', 'short_name': 'Gemma 3 27B Instruct'},\n",
       " {'id_name': 'qwen3-32b', 'short_name': 'Qwen 3 32B'},\n",
       " {'id_name': 'qwen3-30b-a3b-thinking-2507',\n",
       "  'short_name': 'Qwen 3 30B A3B Thinking 2507'},\n",
       " {'id_name': 'qwen3-235b-a22b', 'short_name': 'Qwen 3 235B A22B 2507'},\n",
       " {'id_name': 'llama-3.3-70b-instruct',\n",
       "  'short_name': 'Meta Llama 3.3 70B Instruct'},\n",
       " {'id_name': 'qwen2.5-vl-72b-instruct',\n",
       "  'short_name': 'Qwen 2.5 VL 72B Instruct'},\n",
       " {'id_name': 'medgemma-27b-it', 'short_name': 'MedGemma 27B Instruct'},\n",
       " {'id_name': 'qwq-32b', 'short_name': 'Qwen QwQ 32B'},\n",
       " {'id_name': 'deepseek-r1', 'short_name': 'DeepSeek R1 0528'},\n",
       " {'id_name': 'deepseek-r1-distill-llama-70b',\n",
       "  'short_name': 'DeepSeek R1 Distill Llama 70B'},\n",
       " {'id_name': 'mistral-large-instruct', 'short_name': 'Mistral Large Instruct'},\n",
       " {'id_name': 'qwen2.5-coder-32b-instruct',\n",
       "  'short_name': 'Qwen 2.5 Coder 32B Instruct'},\n",
       " {'id_name': 'internvl2.5-8b', 'short_name': 'InternVL2.5 8B MPO'},\n",
       " {'id_name': 'teuken-7b-instruct-research',\n",
       "  'short_name': 'Teuken 7B Instruct Research'},\n",
       " {'id_name': 'codestral-22b', 'short_name': 'Codestral 22B'},\n",
       " {'id_name': 'llama-3.1-sauerkrautlm-70b-instruct',\n",
       "  'short_name': 'Llama 3.1 SauerkrautLM 70B Instruct'},\n",
       " {'id_name': 'meta-llama-3.1-8b-rag', 'short_name': 'Meta Llama 3.1 8B RAG'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Definition of get_llm_from_api function. Gives back list of all available LLMs from KISSKI\n",
    "def get_llms_from_api():\n",
    "    \"\"\"\n",
    "    Get all LLMs from the API.\n",
    "    \"\"\"\n",
    "    client = OpenAI(\n",
    "        api_key = \"f00b07316f6f7f5e2f1519e7be703dba\", base_url=\"https://chat-ai.academiccloud.de/v1/\"\n",
    "    )\n",
    "    response = client.models.list()\n",
    "    llms = []\n",
    "    for model in response.data:\n",
    "        llms.append(\n",
    "            {\n",
    "                \"id_name\": model.id,\n",
    "                \"short_name\": model.name,\n",
    "            }\n",
    "        )\n",
    "    return llms\n",
    "\n",
    "#Ausführen der Funktion\n",
    "get_llms_from_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cadf86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logger and get_llm_response function\n",
    "logger = logging.getLogger('test_logger')\n",
    "logger.setLevel(logging.DEBUG) # Set logging level to DEBUG for more detailed output\n",
    "\n",
    "\n",
    "# Definition of get_llm_response function\n",
    "def get_llm_response(\n",
    "    messages: list,\n",
    "    model=\"meta-llama-3.1-8b-instruct\",\n",
    "    temperature=0.7,\n",
    "    key=None,\n",
    "    max_tokens=1000,\n",
    "    url=None,\n",
    "):\n",
    "    logger.debug(\"Starting get_llm_response function.\")\n",
    "\n",
    "    # Set API credentials\n",
    "    try:\n",
    "        logger.debug(\"Attempting to get API key and URL.\")\n",
    "        if key is None:\n",
    "            key = \"f00b07316f6f7f5e2f1519e7be703dba\"\n",
    "            logger.debug(f\"Retrieved key from userdata: {'Key found' if key else 'Key not found'}\")\n",
    "        else:\n",
    "            logger.debug(\"Key provided directly.\")\n",
    "\n",
    "        if url is None:\n",
    "            url = \"https://chat-ai.academiccloud.de/v1/\" # Replace with your env var name\n",
    "            logger.debug(f\"Using default URL: {url}\")\n",
    "        else:\n",
    "             logger.debug(f\"URL provided directly: {url}\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"KeyError when getting API key or URL: {e}\")\n",
    "        return \"I'm sorry, but I couldn't process your request.\", {}\n",
    "\n",
    "    if not key:\n",
    "        logger.error(\"API key is missing.\")\n",
    "        return \"I'm sorry, but the API key is not set.\", {}\n",
    "\n",
    "    logger.debug(f\"API Key status: {'Set' if key else 'Not Set'}\")\n",
    "    logger.debug(f\"Base URL: {url}\")\n",
    "\n",
    "    # Initialize client\n",
    "    try:\n",
    "        logger.debug(\"Initializing OpenAI client.\")\n",
    "        client = OpenAI(\n",
    "            api_key=key,\n",
    "            base_url=url,\n",
    "            timeout=httpx.Timeout(60.0, connect=10.0)\n",
    "        )\n",
    "        logger.debug(\"OpenAI client initialized successfully.\")\n",
    "    except Exception as e:\n",
    "         logger.error(f\"Error initializing OpenAI client: {e}\")\n",
    "         return \"I'm sorry, there was an error initializing the API client.\", {}\n",
    "\n",
    "\n",
    "    metadata = {}\n",
    "\n",
    "    # Make API request\n",
    "    try:\n",
    "        logger.debug(\"Making API request.\")\n",
    "        start_time = time.perf_counter()\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        end_time = time.perf_counter()\n",
    "        duration = end_time - start_time\n",
    "        logger.debug(\"API request completed.\")\n",
    "\n",
    "        # Extract response data\n",
    "        response_content = response.choices[0].message.content\n",
    "        prompt_tokens = response.usage.prompt_tokens if response.usage else 'N/A'\n",
    "        generated_tokens = response.usage.completion_tokens if response.usage else 'N/A'\n",
    "\n",
    "        # Log information\n",
    "        logger.info(f\"LLM response time: {duration:.4f} seconds\")\n",
    "        logger.debug(f\"Prompt tokens: {prompt_tokens}, Generated tokens: {generated_tokens}\")\n",
    "\n",
    "        # Create metadata dictionary\n",
    "        metadata = {\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"generated_tokens\": generated_tokens,\n",
    "            \"duration\": duration,\n",
    "        }\n",
    "        logger.debug(\"Metadata created.\")\n",
    "\n",
    "        return response_content, metadata\n",
    "    except OpenAIError as e:\n",
    "        logger.error(f\"An OpenAIError occurred while calling the LLM API: {e}\")\n",
    "        return \"I'm sorry, but I couldn't process your request due to an API error.\", metadata\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during the API call: {e}\")\n",
    "        return \"I'm sorry, an unexpected error occurred.\", metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded97c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting recording for 15 seconds...\n",
      "Recording saved successfully as meeting.wav\n",
      "FFmpeg output:\n",
      "\n",
      "ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers\n",
      "  built with Apple clang version 16.0.0 (clang-1600.0.26.6)\n",
      "  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1.1_1 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.101 / 61. 19.101\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "2025-09-03 10:36:03.321 ffmpeg[97961:1201988] WARNING: Add NSCameraUseContinuityCameraDeviceType to your Info.plist to use AVCaptureDeviceTypeContinuityCamera.\n",
      "2025-09-03 10:36:03.579 ffmpeg[97961:1201988] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "Input #0, avfoundation, from ':0':\n",
      "  Duration: N/A, start: 50189.794000, bitrate: 1536 kb/s\n",
      "  Stream #0:0: Audio: pcm_f32le, 48000 Hz, mono, flt, 1536 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_f32le (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'meeting.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf61.7.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, mono, s16, 768 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.19.101 pcm_s16le\n",
      "size=      42KiB time=00:00:00.51 bitrate= 673.2kbits/s speed=1.02x    \n",
      "size=      83KiB time=00:00:01.00 bitrate= 678.7kbits/s speed=0.994x    \n",
      "size=     125KiB time=00:00:01.51 bitrate= 676.5kbits/s speed=   1x    \n",
      "size=     167KiB time=00:00:02.02 bitrate= 675.3kbits/s speed=   1x    \n",
      "size=     209KiB time=00:00:02.52 bitrate= 677.5kbits/s speed=   1x    \n",
      "size=     250KiB time=00:00:03.02 bitrate= 676.3kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:03.53 bitrate= 594.0kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:04.03 bitrate= 520.1kbits/s speed=0.999x    \n",
      "size=     256KiB time=00:00:04.54 bitrate= 461.5kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:05.04 bitrate= 415.7kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:05.54 bitrate= 378.1kbits/s speed=   1x    \n",
      "size=     256KiB time=00:00:06.05 bitrate= 346.1kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:06.54 bitrate= 640.4kbits/s speed=0.999x    \n",
      "size=     512KiB time=00:00:07.07 bitrate= 593.1kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:07.57 bitrate= 553.8kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:08.07 bitrate= 519.4kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:08.57 bitrate= 489.1kbits/s speed=   1x    \n",
      "size=     512KiB time=00:00:09.07 bitrate= 462.1kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:09.57 bitrate= 656.8kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:10.09 bitrate= 623.5kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:10.59 bitrate= 594.0kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:11.09 bitrate= 567.1kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:11.59 bitrate= 542.6kbits/s speed=   1x    \n",
      "size=     768KiB time=00:00:12.10 bitrate= 519.7kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:12.60 bitrate= 665.3kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:13.10 bitrate= 639.9kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:13.61 bitrate= 616.3kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:14.12 bitrate= 594.0kbits/s speed=   1x    \n",
      "size=    1024KiB time=00:00:14.62 bitrate= 573.6kbits/s speed=   1x    \n",
      "[out#0/wav @ 0x11c762000] video:0KiB audio:1253KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.006078%\n",
      "size=    1253KiB time=00:00:15.00 bitrate= 684.5kbits/s speed=0.999x    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Definition of record_audio (once) function \n",
    "\n",
    "def record_audio(output_filename, duration=15):\n",
    "    \"\"\"\n",
    "    Records audio from the microphone for a given duration and saves it to a file.\n",
    "\n",
    "    Args:\n",
    "        output_filename (str): The name of the output .wav file.\n",
    "        duration (int): The duration of the recording in seconds.\n",
    "    \"\"\"\n",
    "    print(f\"Starting recording for {duration} seconds...\")\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-f', 'avfoundation',  # Use AVFoundation for macOS\n",
    "        '-i', ':0',            # Select the default audio device\n",
    "        '-t', str(duration),   # Set the recording duration\n",
    "        '-y',                  # Overwrite output file if it exists\n",
    "        output_filename\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        print(f\"Recording saved successfully as {output_filename}\")\n",
    "        print(\"FFmpeg output:\")\n",
    "        print(result.stdout)\n",
    "        print(result.stderr)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred during recording: {e}\")\n",
    "        print(\"FFmpeg output (stderr):\")\n",
    "        print(e.stderr)\n",
    "    except FileNotFoundError:\n",
    "        print(\"ffmpeg not found. Please ensure ffmpeg is installed and in your PATH.\")\n",
    "\n",
    "# Record 10 seconds of audio and save it as 'meeting.wav'\n",
    "record_audio('meeting.wav', duration=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fccf0a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription Result:\n",
      "{'transcription': ' über den Airports?  Okay.  Legen eine Airports in das Labelkäse.  Sie ist in der Kühlenfahrt und 30 Sekunden.  2.2.  Auf einem iPhone-Liefer.  Das sind deinen Airports-Gegoppel.  Das ist die Option.  Ein Stellung.  Nun schluss auf.', 'language': 'de', 'confidence': 0.945986270904541}\n"
     ]
    }
   ],
   "source": [
    "#Transcribe Audio with STT-Service with Docker Container, definition and execution\n",
    "def transcribe_audio(file_path):\n",
    "    \"\"\"\n",
    "    Sends an audio file to the STT service and returns the transcription.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the audio file.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Attempting to transcribe audio file: {file_path}\")\n",
    "    \n",
    "    # The URL of the transcription service in the Docker container\n",
    "    transcribe_url = \"http://localhost:8080/transcribe/file\"\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"Audio file not found at: {file_path}\")\n",
    "            return \"Error: Audio file not found.\"\n",
    "\n",
    "        with open(file_path, \"rb\") as audio_file:\n",
    "            files = {\"file\": (os.path.basename(file_path), audio_file, \"audio/wav\")}\n",
    "            \n",
    "            logger.debug(f\"Sending POST request to {transcribe_url}\")\n",
    "            with httpx.Client() as client:\n",
    "                response = client.post(transcribe_url, files=files, timeout=60)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            transcription = response.json()\n",
    "            logger.info(\"Successfully received transcription.\")\n",
    "            logger.debug(f\"Transcription result: {transcription}\")\n",
    "            return transcription\n",
    "\n",
    "    except httpx.RequestError as e:\n",
    "        logger.error(f\"An error occurred while requesting transcription: {e}\")\n",
    "        return f\"Error connecting to the transcription service: {e}\"\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        logger.error(f\"Received an HTTP error: {e.response.status_code} - {e.response.text}\")\n",
    "        return f\"HTTP Error: {e.response.status_code}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "#transcribe the audio recording\n",
    "audio_to_transcribe = 'meeting.wav'\n",
    "transcription_result = transcribe_audio(audio_to_transcribe)\n",
    "\n",
    "print(\"Transcription Result:\")\n",
    "print(transcription_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770fe382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Originaltext that was transcribed\n",
    "original_text = '''Um besser zu verstehen, welche Vorteile dir Django bietet, werfen wir einen Blick auf Server im Allgemeinen. \n",
    "Als Erstes muss der Server wissen, dass er eine Webseite ausliefern soll. Der Server hat mehrere \"Ports\". \n",
    "Ein Port ist vergleichbar mit einem Briefkasten, der auf eingehende Briefe antwortet.'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472aad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:\n",
      "Ein Server benötigt Informationen, um eine Website auszuliefern. Er hat verschiedene Ports, die wie Briefkästen funktionieren, die auf bestimmte Anfragen reagieren.\n",
      "{'prompt_tokens': 162, 'generated_tokens': 41, 'duration': 1.1010920840017207}\n"
     ]
    }
   ],
   "source": [
    "# Erstellen Sie die Nachrichten für die LLM-API\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Sie sind ein hilfreicher Assistent, der Texte zusammenfasst.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Fassen Sie den folgenden Text zusammen:\\n\\n {transcription_result} \"}\n",
    "]\n",
    "\n",
    "# Rufen Sie die get_llm_response Funktion auf (aus der Zelle oben)\n",
    "summary, metadata = get_llm_response(messages)\n",
    "\n",
    "# Geben Sie die Zusammenfassung aus\n",
    "print(\"Zusammenfassung:\")\n",
    "print(summary)\n",
    "print(metadata)\n",
    "\n",
    "# Optional: Metadaten anzeigen\n",
    "# print(\"\\nMetadaten:\")\n",
    "# display(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c48c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Summary:\n",
      "Zusammenfassung:\n",
      "\n",
      "Ein Webserver ist für die Verbindung zwischen Benutzer und Website verantwortlich. Er nimmt Anfragen (Request) entgegen und antwortet mit der Website (Response). Um die Website bereitzustellen, benötigt der Webserver jedoch Inhalte, die durch eine Framework wie Django erstellt werden können. Django hilft bei der Erstellung von Inhalten, um die Website nutzbar zu machen.\n"
     ]
    }
   ],
   "source": [
    "# Initialize rolling summary\n",
    "rolling_summary = \"\"\n",
    "\n",
    "# After each new transcription, update the rolling summary\n",
    "def update_rolling_summary(new_transcription, previous_summary):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent, der fortlaufende Zusammenfassungen erstellt.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Vorherige Zusammenfassung:\\n{previous_summary}\\n\\nNeue Nachricht:\\n{new_transcription}\\n\\nFasse alles zusammen.\"}\n",
    "    ]\n",
    "    summary, metadata = get_llm_response(messages)\n",
    "    return summary\n",
    "\n",
    "# Example usage after getting a new transcription\n",
    "new_transcription = transcription_result.get('transcription', '')\n",
    "rolling_summary = update_rolling_summary(new_transcription, rolling_summary)\n",
    "\n",
    "print(\"Rolling Summary:\")\n",
    "print(rolling_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d541e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neue Nachricht:\n",
      " Es gibt Anzeichen dafür, dass Menschen auf dem Wohnungsmarkt in Göttingen rassistisch diskriminiert werden. Eine Studie hat festgestellt, dass Menschen mit fiktiven Namen, die unterschiedliche Herkunftsrückgründe haben, unterschiedlich behandelt werden. Während Josef Eilgen, ein Name mit möglicherweise nicht deutscher Herkunft, seltener zu Wohnungsbesichtigungen eingeladen wird, wird Jakob Schütte, ein Name mit deutscher Herkunft, häufiger eingeladen. Diese Ergebnisse deuten darauf hin, dass rassistische Diskriminierung vorliegt, was illegal ist.\n"
     ]
    }
   ],
   "source": [
    "#Pseudoaudiostreaming with proper logging\n",
    "def record_chunk(filename, duration):\n",
    "    \"\"\"\n",
    "    Records a short chunk of audio using ffmpeg.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Output filename for the audio chunk\n",
    "        duration (int): Duration in seconds to record\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting recording of {duration}s audio chunk to {filename}\")\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-f', 'avfoundation',  # macOS\n",
    "        '-i', ':0',\n",
    "        '-t', str(duration),\n",
    "        '-y',\n",
    "        filename\n",
    "    ]\n",
    "    try:\n",
    "        logger.debug(f\"Executing ffmpeg command: {' '.join(command)}\")\n",
    "        subprocess.run(command, check=True, capture_output=True)\n",
    "        logger.info(f\"Successfully recorded audio chunk to {filename}\")\n",
    "    except subprocess.SubprocessError as e:\n",
    "        logger.error(f\"Error recording audio chunk: {e}\")\n",
    "        raise\n",
    "\n",
    "def transcribe_chunk(file_path):\n",
    "    \"\"\"\n",
    "    Sends an audio chunk to the transcription service.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the audio file to transcribe\n",
    "        \n",
    "    Returns:\n",
    "        str: The transcribed text\n",
    "    \"\"\"\n",
    "    logger.info(f\"Transcribing audio chunk: {file_path}\")\n",
    "    url = \"http://localhost:8080/transcribe/file\"\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.error(f\"Audio file not found: {file_path}\")\n",
    "            return \"Error: Audio file not found.\"\n",
    "            \n",
    "        with open(file_path, \"rb\") as audio_file:\n",
    "            files = {\"file\": (os.path.basename(file_path), audio_file, \"audio/wav\")}\n",
    "            \n",
    "            logger.debug(f\"Sending POST request to {url}\")\n",
    "            with httpx.Client() as client:\n",
    "                response = client.post(url, files=files, timeout=60)\n",
    "            \n",
    "            # Check if the request was successful\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            transcription = response.json().get('transcription', '')\n",
    "            logger.info(f\"Successfully transcribed chunk: {file_path}\")\n",
    "            logger.debug(f\"Transcription result: {transcription[:50]}...\" if len(transcription) > 50 else transcription)\n",
    "            return transcription\n",
    "            \n",
    "    except httpx.RequestError as e:\n",
    "        logger.error(f\"Request error during transcription: {e}\")\n",
    "        return f\"Error connecting to the transcription service: {e}\"\n",
    "    except httpx.HTTPStatusError as e:\n",
    "        logger.error(f\"HTTP error during transcription: {e.response.status_code} - {e.response.text}\")\n",
    "        return f\"HTTP Error: {e.response.status_code}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error during transcription: {e}\")\n",
    "        return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "def pseudo_streaming(max_chunks=None, chunk_duration=10):\n",
    "\n",
    "    logger.info(\"Starting pseudo-streaming audio transcription service\")\n",
    "    chunk_queue = Queue()\n",
    "    stop_event = threading.Event()\n",
    "    chunk_idx = 0\n",
    "    rolling_summary = \"\"\n",
    "    recorder_exception = None\n",
    "    worker_exception = None\n",
    "    \n",
    "    def recorder():\n",
    "        nonlocal recorder_exception\n",
    "        try:\n",
    "            idx = 0\n",
    "            while not stop_event.is_set() and (max_chunks is None or idx < max_chunks):\n",
    "                chunk_file = f\"chunk_{idx}.wav\"\n",
    "                logger.info(f\"Processing chunk {chunk_file}\")\n",
    "                try: \n",
    "                    record_chunk(chunk_file, duration=chunk_duration)\n",
    "                    chunk_queue.put(chunk_file)\n",
    "                    logger.debug(f\"[recorder] Put {chunk_file} in queue\")\n",
    "                except Exception as e: \n",
    "                    logger.error(f\"[recorder] Error recording chunk {idx}: {e}\")\n",
    "                    recorder_exception = e\n",
    "                    stop_event.set()\n",
    "                    break\n",
    "                idx += 1 \n",
    "            logger.info(\"[recorder] Finished recording loop\")\n",
    "        except Exception as e:\n",
    "            recorder_exception = e \n",
    "            stop_event.set()\n",
    "            logger.exception(\"[recorder] Unhandled exception\")\n",
    "                \n",
    "    def worker():\n",
    "        nonlocal rolling_summary, worker_exception\n",
    "        processed = 0\n",
    "        try:\n",
    "            while not stop_event.is_set():\n",
    "                try:\n",
    "                    chunk_file = chunk_queue.get(timeout=2)\n",
    "                except Exception:\n",
    "                    # timeout, check termination condition\n",
    "                    if max_chunks is not None and processed >= max_chunks:\n",
    "                        logger.debug(\"[worker] No more expected chunks and processed reached max_chunks\")\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "                logger.info(f\"[worker] Transcribing {chunk_file}\")\n",
    "                try:\n",
    "                    transcription = transcribe_chunk(chunk_file)\n",
    "                    # append to full transcription\n",
    "                    with open(\"full_transcription.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                        f.write(transcription + \"\\n\")\n",
    "                    # update rolling summary with LLM\n",
    "                    messages = [\n",
    "                        {\"role\": \"system\", \"content\": \"Fasse den erhaltenen Text zusammen. Falls eine Vorherige Zusammenfassung existiert, ergänze sie um die Neue Nachricht. Falls keine Vorherige Zusammenfassung existiert, beginne eine Neue. Fasse den Inhalt so kurz wie möglich zusammen\"},\n",
    "                        {\"role\": \"user\", \"content\": f\"Vorherige Zusammenfassung:\\n{rolling_summary}\\n\\nNeue Nachricht:\\n{transcription}\\n\\n.\"}\n",
    "                    ]\n",
    "                    summary, metadata = get_llm_response(messages)\n",
    "                    rolling_summary = summary\n",
    "                    logger.info(f\"[worker] Updated rolling summary (chunks processed: {processed+1})\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"[worker] Error processing {chunk_file}: {e}\")\n",
    "                    worker_exception = e\n",
    "                finally: \n",
    "                    #cleanup \n",
    "                    try: \n",
    "                        if os.path.exists(chunk_file):\n",
    "                            os.remove(chunk_file)\n",
    "                            logger.debug(f\"[worker] Removed temporary chunk file {chunk_file}\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"[worker] Error cleaning up {chunk_file}: {e}\")\n",
    "                    processed += 1\n",
    "                    chunk_queue.task_done()\n",
    "                \n",
    "                if max_chunks is not None and processed >= max_chunks:\n",
    "                    logger.debug(\"[worker] Reached max_chunks, stopping\")\n",
    "                    stop_event.set()\n",
    "                    break\n",
    "            logger.info(\"[worker] Exiting worker loop\")        \n",
    "        except Exception as e:\n",
    "            worker_exception = e\n",
    "            stop_event.set()\n",
    "            logger.exception(\"[worker] Unhandled exception\")\n",
    "\n",
    "    # Start threads inside the function\n",
    "    rec_thread = threading.Thread(target=recorder, name=\"recorder_thread\", daemon=True)\n",
    "    work_thread = threading.Thread(target=worker, name=\"worker_thread\", daemon=True)\n",
    "    rec_thread.start()\n",
    "    work_thread.start()\n",
    "\n",
    "    try: \n",
    "        while work_thread.is_alive():\n",
    "            work_thread.join(timeout=1)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Pseudo_streaming stopped by user via KeyboardInterrupt\")\n",
    "        stop_event.set()\n",
    "\n",
    "    stop_event.set()\n",
    "    rec_thread.join(timeout=2)\n",
    "    work_thread.join(timeout=2)\n",
    "\n",
    "    if recorder_exception: \n",
    "        logger.error(f\"Recorder error occured: {recorder_exception}\")\n",
    "    if worker_exception: \n",
    "        logger.error(f\"Worker error occured: {worker_exception}\")\n",
    "\n",
    "    logger.info(\"Pseudo_streaming ended\")\n",
    "    return rolling_summary\n",
    "\n",
    "\n",
    "result = pseudo_streaming(max_chunks=2, chunk_duration=10)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record audio chunk\n",
    "                record_chunk(chunk_file, duration=10)\n",
    "                \n",
    "                # Transcribe the chunk\n",
    "                transcription = transcribe_chunk(chunk_file)\n",
    "            \n",
    "                with open(\"full_transcription.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(transcription + \"\\n\")    \n",
    "\n",
    "                # Clean up\n",
    "                logger.debug(f\"Removing temporary file: {chunk_file}\")\n",
    "                os.remove(chunk_file)\n",
    "                \n",
    "            \n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"Fasse den erhaltenen Text zusammen. Falls eine Vorherige Zusammenfassung existiert, ergänze sie um die Neue Nachricht. Falls keine Vorherige Zusammenfassung existiert, beginne eine Neue.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Vorherige Zusammenfassung:\\n{rolling_summary}\\n\\nNeue Nachricht:\\n{transcription}\\n\\n.\"}\n",
    "                ]\n",
    "                rolling_summary, metadata = get_llm_response(messages)\n",
    "                #print(\"Zusammenfassung\")\n",
    "                print(rolling_summary)\n",
    "                #print(metadata)\n",
    "\n",
    "                # Increment for next iteration\n",
    "                chunk_idx += \n",
    "\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Pseudo-streaming stopped by user\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in pseudo-streaming: {e}\")\n",
    "        finally:\n",
    "            logger.info(f\"Pseudo-streaming ended after processing {chunk_idx} chunks\")\n",
    "            return None #rolling_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
